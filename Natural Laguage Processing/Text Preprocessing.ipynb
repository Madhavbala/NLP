{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cdb4ed9",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Sequencing\n",
    "- Padding\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7f3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d968a",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc48884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "428d1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['We love machine learning']\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0170c9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'we': 1, 'love': 2, 'machine': 3, 'learning': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b18110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 1, 'we': 2, 'love': 3, 'machine': 4, 'and': 5, 'deep': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeating words\n",
    "sentence = ['We love machine learning and deep learning']\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16602c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 1, 'we': 2, 'love': 3, 'machine': 4, 'and': 5, 'deep': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization is not case sensitive\n",
    "# Tokenization does not consider special characters\n",
    "sentence = ['@ We love machine LEARNING...,!! and deep learning']\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "746cc3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'we': 1,\n",
       " 'are': 2,\n",
       " 'learning': 3,\n",
       " 'natural': 4,\n",
       " 'language': 5,\n",
       " 'processing': 6,\n",
       " 'have': 7,\n",
       " 'learned': 8,\n",
       " 'computer': 9,\n",
       " 'vision': 10,\n",
       " 'text': 11,\n",
       " 'preprocessing': 12,\n",
       " 'from': 13,\n",
       " 'a': 14,\n",
       " 'good': 15,\n",
       " 'trainer': 16}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['We are learning natural language processing',\n",
    "            'We have learned computer vision',\n",
    "            'we are learning text preprocessing',\n",
    "            'we are learning from a good trainer']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3dbd0",
   "metadata": {},
   "source": [
    "---\n",
    "## Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf53fbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 1,\n",
       " 'refers': 2,\n",
       " 'representing': 3,\n",
       " 'as': 4,\n",
       " 'of': 5,\n",
       " 'we': 6,\n",
       " 'are': 7,\n",
       " 'learning': 8,\n",
       " 'text': 9,\n",
       " 'preprocessing': 10,\n",
       " 'tokenization': 11,\n",
       " 'each': 12,\n",
       " 'word': 13,\n",
       " 'a': 14,\n",
       " 'token': 15,\n",
       " 'sequencing': 16,\n",
       " 'sentences': 17,\n",
       " 'sequence': 18,\n",
       " 'tokens': 19,\n",
       " 'padding': 20,\n",
       " 'adding': 21,\n",
       " 'zeros': 22,\n",
       " 'sequences': 23,\n",
       " 'make': 24,\n",
       " 'them': 25,\n",
       " 'all': 26,\n",
       " 'same': 27,\n",
       " 'length': 28}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['We are learning text preprocessing',\n",
    "             'Tokenization refers to representing each word as a token',\n",
    "             'Sequencing refers to representing sentences as sequence of tokens',\n",
    "             'Padding refers to adding zeros to sequences to make them all of same length']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "861ecf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7, 8, 9, 10],\n",
       " [11, 2, 1, 3, 12, 13, 4, 14, 15],\n",
       " [16, 2, 1, 3, 17, 4, 18, 5, 19],\n",
       " [20, 2, 1, 21, 22, 1, 23, 1, 24, 25, 26, 5, 27, 28]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d4eca51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 10, 11, 16, 20]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Text preprocessing involves tokenization, sequencing and padding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38a3ed69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 10, 11, 16, 20]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Text preprocessing does not involve tokenization, sequencing and padding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea355694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#OOV': 1,\n",
       " 'to': 2,\n",
       " 'refers': 3,\n",
       " 'representing': 4,\n",
       " 'as': 5,\n",
       " 'of': 6,\n",
       " 'we': 7,\n",
       " 'are': 8,\n",
       " 'learning': 9,\n",
       " 'text': 10,\n",
       " 'preprocessing': 11,\n",
       " 'tokenization': 12,\n",
       " 'each': 13,\n",
       " 'word': 14,\n",
       " 'a': 15,\n",
       " 'token': 16,\n",
       " 'sequencing': 17,\n",
       " 'sentences': 18,\n",
       " 'sequence': 19,\n",
       " 'tokens': 20,\n",
       " 'padding': 21,\n",
       " 'adding': 22,\n",
       " 'zeros': 23,\n",
       " 'sequences': 24,\n",
       " 'make': 25,\n",
       " 'them': 26,\n",
       " 'all': 27,\n",
       " 'same': 28,\n",
       " 'length': 29}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out of vocabulary token\n",
    "sentences = ['We are learning text preprocessing',\n",
    "             'Tokenization refers to representing each word as a token',\n",
    "             'Sequencing refers to representing sentences as sequence of tokens',\n",
    "             'Padding refers to adding zeros to sequences to make them all of same length']\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = '#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f09e27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 8, 9, 10, 11],\n",
       " [12, 3, 2, 4, 13, 14, 5, 15, 16],\n",
       " [17, 3, 2, 4, 18, 5, 19, 6, 20],\n",
       " [21, 3, 2, 22, 23, 2, 24, 2, 25, 26, 27, 6, 28, 29]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1424ffcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 11, 1, 12, 17, 1, 21]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Text preprocessing involves tokenization, sequencing and padding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e6bbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 11, 1, 1, 1, 12, 17, 1, 21]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Text preprocessing does not involve tokenization, sequencing and padding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71188a72",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef13848b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 6, 5, 2],\n",
       " [3, 4, 2, 7],\n",
       " [3, 4, 2, 8],\n",
       " [3, 4, 2, 9, 10, 11, 12],\n",
       " [5, 2, 13, 14, 2, 4, 15]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['We love machine learning',\n",
    "             'We are learning tokenization',\n",
    "             'We are learning sequencing',\n",
    "             'We are learning the technique of padding',\n",
    "             'machine learning and deep learning are fun']\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = '#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29ccde77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  3,  6,  5,  2],\n",
       "       [ 0,  0,  0,  3,  4,  2,  7],\n",
       "       [ 0,  0,  0,  3,  4,  2,  8],\n",
       "       [ 3,  4,  2,  9, 10, 11, 12],\n",
       "       [ 5,  2, 13, 14,  2,  4, 15]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_sequences = pad_sequences(sequences)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15ef6e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  3,  6,  5,  2],\n",
       "       [ 0,  0,  0,  3,  4,  2,  7],\n",
       "       [ 0,  0,  0,  3,  4,  2,  8],\n",
       "       [ 3,  4,  2,  9, 10, 11, 12],\n",
       "       [ 5,  2, 13, 14,  2,  4, 15]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding = 'pre')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "692ff955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  6,  5,  2,  0,  0,  0],\n",
       "       [ 3,  4,  2,  7,  0,  0,  0],\n",
       "       [ 3,  4,  2,  8,  0,  0,  0],\n",
       "       [ 3,  4,  2,  9, 10, 11, 12],\n",
       "       [ 5,  2, 13, 14,  2,  4, 15]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding = 'post')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6058e965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9, 6, 2],\n",
       " [3, 4, 2, 10],\n",
       " [3, 4, 2, 11],\n",
       " [3, 4, 2, 5, 12, 13, 14],\n",
       " [6, 2, 15, 16, 2, 4, 17],\n",
       " [5, 18, 19, 20, 7, 21, 22, 8, 23, 24, 25, 8, 5, 7, 26]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['We love machine learning',\n",
    "             'We are learning tokenization',\n",
    "             'We are learning sequencing',\n",
    "             'We are learning the technique of padding',\n",
    "             'Machine learning and deep learning are fun',\n",
    "             'The main goal behind text preprocessing is to give numerical representation to the text data']\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = '#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52ebd37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  9,  6,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  4,  2, 10],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  4,  2, 11],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  3,  4,  2,  5, 12, 13, 14],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  6,  2, 15, 16,  2,  4, 17],\n",
       "       [ 5, 18, 19, 20,  7, 21, 22,  8, 23, 24, 25,  8,  5,  7, 26]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding = 'pre')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa09d945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  3,  9,  6,  2],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 10],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 11],\n",
       "       [ 0,  3,  4,  2,  5, 12, 13, 14],\n",
       "       [ 0,  6,  2, 15, 16,  2,  4, 17],\n",
       "       [ 8, 23, 24, 25,  8,  5,  7, 26]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding = 'pre', maxlen = 8)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7772ca50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  3,  9,  6,  2],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 10],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 11],\n",
       "       [ 0,  3,  4,  2,  5, 12, 13, 14],\n",
       "       [ 0,  6,  2, 15, 16,  2,  4, 17],\n",
       "       [ 8, 23, 24, 25,  8,  5,  7, 26]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding = 'pre', maxlen = 8, truncating = 'pre')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3fc9eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  3,  9,  6,  2],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 10],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 11],\n",
       "       [ 0,  3,  4,  2,  5, 12, 13, 14],\n",
       "       [ 0,  6,  2, 15, 16,  2,  4, 17],\n",
       "       [ 5, 18, 19, 20,  7, 21, 22,  8]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding = 'pre', maxlen = 8, truncating = 'post')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260c621",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78a9df8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  3,  9,  6,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  4,  2, 10],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  4,  2, 11],\n",
       "       [ 0,  0,  0,  3,  4,  2,  5, 12, 13, 14],\n",
       "       [ 0,  0,  0,  6,  2, 15, 16,  2,  4, 17],\n",
       "       [21, 22,  8, 23, 24, 25,  8,  5,  7, 26]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete process\n",
    "sentences = ['We love machine learning',\n",
    "             'We are learning tokenization',\n",
    "             'We are learning sequencing',\n",
    "             'We are learning the technique of padding',\n",
    "             'Machine learning and deep learning are fun',\n",
    "             'The main goal behind text preprocessing is to give numerical representation to the text data']\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = '#OOV')                 # Create a tokenizer object\n",
    "tokenizer.fit_on_texts(sentences)                         # Tokenization\n",
    "sequences = tokenizer.texts_to_sequences(sentences)       # Sequencing\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 10)  # Padding\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c0f870",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfacb2",
   "metadata": {},
   "source": [
    "---\n",
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecc8233a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'break'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('breaking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b897e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n",
      "break\n",
      "broke\n",
      "broken\n",
      "chang\n",
      "chang\n",
      "chang\n",
      "write\n",
      "write\n",
      "run\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "ran\n",
      "cat\n",
      "knive\n",
      "leav\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('breaks'))\n",
    "print(stemmer.stem('breaking'))\n",
    "print(stemmer.stem('broke'))\n",
    "print(stemmer.stem('broken'))\n",
    "print(stemmer.stem('changes'))\n",
    "print(stemmer.stem('changed'))\n",
    "print(stemmer.stem('changing'))\n",
    "print(stemmer.stem('writes'))\n",
    "print(stemmer.stem('writing'))\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('trouble'))\n",
    "print(stemmer.stem('troubling'))\n",
    "print(stemmer.stem('troubled'))\n",
    "print(stemmer.stem('ran'))\n",
    "print(stemmer.stem('cats'))\n",
    "print(stemmer.stem('knives'))\n",
    "print(stemmer.stem('leaves'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24386d15",
   "metadata": {},
   "source": [
    "---\n",
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80f8ec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cab6a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'break'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('breaks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dca4be65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n",
      "break\n",
      "break\n",
      "break\n",
      "change\n",
      "change\n",
      "change\n",
      "write\n",
      "write\n",
      "run\n",
      "trouble\n",
      "trouble\n",
      "trouble\n",
      "run\n",
      "cat\n",
      "knife\n",
      "leaf\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('breaks', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('breaking', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('broke', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('broken', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('changes', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('changed', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('changing', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('writes', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('writing', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('running', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('trouble', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('troubling', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('troubled', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('ran', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('cats', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('knives'))\n",
    "print(lemmatizer.lemmatize('leaves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d90bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
